// scraper.js
const axios = require("axios");
const cheerio = require("cheerio");
const llog = require("learninglab-log");
const fs   = require("fs");
const path = require("path");

module.exports = async function scraper(url) {
  try {
    /* â”€â”€â”€â”€â”€â”€â”€ fetch â”€â”€â”€â”€â”€â”€â”€ */
    const { data: html } = await axios.get(url, {
      headers: {
        "User-Agent": "Mozilla/5.0 (compatible; LearningLabBot/1.0)"
      },
      timeout: 20_000
    });

    /* â”€â”€â”€â”€â”€â”€â”€ parse â”€â”€â”€â”€â”€â”€â”€ */
    const $ = cheerio.load(html);
    $("script, style, noscript").remove();

    const title = $("head > title").text().trim();
    const text  = $("body").text().replace(/\s+/g, " ").trim();

    /* â”€â”€â”€â”€â”€â”€â”€ debug logs â”€â”€â”€â”€â”€â”€â”€ */
    llog.yellow(`ðŸ”Ž SCRAPER RAW HTML (${url}) â€” first 20000 chars â†“\n${html
      .slice(0, 20000)
      .replace(/\n+/g, " ")
      .trim()}\nâ€¦`);
    llog.yellow(
      `ðŸ”Ž SCRAPER CLEAN TEXT PREVIEW â€” first 400 chars â†“\n${text.slice(
        0,
        400
      )} â€¦`
    );

    /* Optional: write full HTML to /tmp so you can open in VSÂ Code. */
    // const file = path.join("/tmp", `scraped_${Date.now()}.html`);
    // fs.writeFileSync(file, html);
    // llog.magenta(`Full HTML written to ${file}`);

    return { url, title, text, raw: html };
  } catch (err) {
    console.error(`Error scraping ${url}:`, err);
    throw err;
  }
};
